%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Cleese Assignment (For Students)
% LaTeX Template
% Version 2.0 (27/5/2018)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Author:
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt]{article}

\input{structure.tex} % Include the file specifying the document structure and custom commands

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

% Required
\newcommand{\assignmentQuestionName}{Question} % The word to be used as a prefix to question numbers; example alternatives: Problem, Exercise
\newcommand{\assignmentClass}{CNM} % Course/class
\newcommand{\assignmentTitle}{Acceleration of an AI application} % Assignment title or name
\newcommand{\assignmentAuthorName}{Kevin Jordil \& Olivier D'Ancona} % Student name


% Optional (comment lines to remove)
\newcommand{\assignmentClassInstructor}{Professor: Marina Zapater } % Intructor name/time/description
\newcommand{\assignmentDueDate}{Thursday,\ 02\ February\, 2023} % Due date

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\maketitle % Print the title page

\thispagestyle{empty} % Suppress headers and footers on the title page

\newpage

\assignmentSection{Stage 1 – Choosing an application}

\section{Multi-Layer Perceptron}

\paragraph*{Multi-Layer Perceptron (MLP)}is a type of artificial neural network commonly used for supervised learning tasks such as classification.
It consists of one input layer, some hidden layer and one output layer. In each layer, each neuron is connected to all neurons in the next layer.
The architecture that we built have 3 layers:

\paragraph*{Architecture} our neural network implementation is as follows:

\begin{itemize}
	\item Input layer $I$ of size 784 (28x28 images)
	\item Hidden layer $H$ of size 1000 (number of hidden neurons)
	\item Output layer $O$ of size 10 (number of class of the dataset)
\end{itemize}

We used a c implementation who consists of a single file with all the functions needed to train and test the neural network. The batch size $B=10$.

\paragraph*{Dataset} We used the fashion dataset which is a commonly used dataset in machine learning and computer vision task.
He consists of a collection of Zalando shopping items such as shirts, pants and shoes. There are in total 10 categories.
he class are T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag and Ankle boot.
Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.
Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker.
This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns.
The first column consists of the class labels (see above), and represents the article of clothing.
The rest of the columns contain the pixel-values of the associated image.

\section{Grayscale Conversion Program}

\paragraph*{Definition} We ran into troubles with the cuda neural network acceleration so we developped an auxiliary program to be accelerated with cuda.
The program converts a color image to a grayscale image which is a good idea to implement with our multi layer perceptron.
Because our mlp necessitates a grayscale image as input we can convert rgb images to grayscale images with this program.
Furthermore this task is a good example to show the acceleration of a program with cuda.

\paragraph*{Principle} For each pixel of the image, we calculate the average of all color channels.
The result is the grayscale value of the pixel. Then we replace the color channels with the grayscale value in a new image.

\pagebreak

\assignmentSection{Stage 2 – Analysing application bottlenecks}

\section{Execution time}

\paragraph*{MLP baseline} The baseline execution time is showed on table \ref{tab:mlp_baseline} with a total time of 575.676547s.

\begin{table}[h]	\centering
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\textbf{time} & \textbf{accuracy} & \textbf{error} & \textbf{samples/sec} & \textbf{gflop/s} \\ \hline
		67.270 s      & 40.99\%           & 0.045          & 743.28               & 2.20             \\ \hline
		134.516 s     & 61.63\%           & 0.022          & 743.53               & 2.20             \\ \hline
		201.754 s     & 74.50\%           & 0.019          & 743.64               & 2.20             \\ \hline
		268.999 s     & 82.60\%           & 0.016          & 743.54               & 2.20             \\ \hline
		336.225 s     & 87.68\%           & 0.015          & 743.77               & 2.20             \\ \hline
		403.459 s     & 90.91\%           & 0.013          & 743.67               & 2.20             \\ \hline
		470.688 s     & 93.01\%           & 0.013          & 743.73               & 2.20             \\ \hline
		537.911 s     & 94.43\%           & 0.012          & 743.79               & 2.20             \\ \hline
	\end{tabular}
	\caption{MLP Baseline execution time}
	\label{tab:mlp_baseline}
\end{table}

\paragraph*{MLP accelerated with OpenMP} The accelerated execution time is showed on table \ref{tab:mlp_accelerated} with a total time of 393.993599s.

\begin{table}[h]	\centering
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\textbf{time} & \textbf{accuracy} & \textbf{error} & \textbf{samples/sec} & \textbf{gflop/s} \\ \hline
		46.092 s      & 40.99\%           & 0.045          & 1084.79              & 3.21             \\ \hline
		92.241 s      & 61.63\%           & 0.022          & 1083.43              & 3.20             \\ \hline
		138.231 s     & 74.50\%           & 0.019          & 1087.20              & 3.22             \\ \hline
		184.198 s     & 82.59\%           & 0.016          & 1087.76              & 3.22             \\ \hline
		230.153 s     & 87.68\%           & 0.015          & 1088.01              & 3.22             \\ \hline
		276.135 s     & 90.91\%           & 0.013          & 1087.40              & 3.22             \\ \hline
		322.124 s     & 93.01\%           & 0.013          & 1087.23              & 3.22             \\ \hline
		368.109 s     & 94.43\%           & 0.012          & 1087.32              & 3.22             \\ \hline
	\end{tabular}
	\caption{MLP OpenMP accelerated execution time}
	\label{tab:mlp_accelerated}
\end{table}

\paragraph*{MLP accelerated with OpenMP and CUDA} The accelerated execution time is showed on table \ref{tab:mlp_accelerated_cuda} with a total time of 326.487s.

\begin{table}[h]	\centering
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\textbf{time} & \textbf{accuracy} & \textbf{error} & \textbf{samples/sec} & \textbf{gflop/s} \\ \hline
		64.577 s      & 36.72\%           & 0.061          & 774.27               & 2.29             \\ \hline
		130.425 s     & 57.61\%           & 0.034          & 759.32               & 2.25             \\ \hline
		195.328 s     & 70.85\%           & 0.029          & 770.39               & 2.28             \\ \hline
		261.072 s     & 79.38\%           & 0.025          & 760.52               & 2.25             \\ \hline
		326.487 s     & 84.74\%           & 0.024          & 764.35               & 2.26             \\ \hline
		391.392 s     & 88.17\%           & 0.021          & 770.37               & 2.28             \\ \hline
		455.994 s     & 90.38\%           & 0.021          & 773.96               & 2.29             \\ \hline
		519.951 s     & 91.87\%           & 0.019          & 781.79               & 2.31             \\ \hline
		584.651 s     & 92.86\%           & 0.020          & 772.79               & 2.29             \\ \hline
		649.924 s     & 93.56\%           & 0.018          & 766.02               & 2.27             \\ \hline
		714.872 s     & 94.03\%           & 0.018          & 769.84               & 2.28             \\ \hline
		780.575 s     & 94.48\%           & 0.017          & 761.00               & 2.25             \\ \hline
		845.612 s     & 94.74\%           & 0.016          & 768.80               & 2.27             \\ \hline
	\end{tabular}
	\caption{MLP OpenMP and CUDA accelerated execution time}
	\label{tab:mlp_accelerated_cuda}
\end{table}

\paragraph*{Grayscale baseline} In the following results, the image name contains the resolution in pixels. Here is the baseline execution time:

\begin{itemize}
	\item ./main chicky\_512\_512.png -> Time elapsed: 3.431923 ms
	\item ./main avatar\_5000\_2381.jpg -> Time elapsed: 151.122006 ms
	\item ./main winter\_10667\_6000.jpg -> Time elapsed: 803.954043 ms
\end{itemize}

\paragraph*{Grayscale OpenMP} The following results use the same images but with CUDA acceleration:

\begin{itemize}
	\item	./main chicky\_512\_512.png -> Time elapsed: 2.095706 ms
	\item	./main avatar\_5000\_2381.jpg -> Time elapsed: 61.278724 ms
	\item	./main winter\_10667\_6000.jpg -> Time elapsed: 209.700440 ms
\end{itemize}

\section{Complexity Analysis}

\paragraph*{MLP} The MLP is a feedforward neural network, so the complexity is bounded by the biggest loop, which is the backpropagation loop in $O(\#epoch  \cdot H \cdot O \cdot B)$

\paragraph*{Grayscale} The complexity is one computation for every pixel so $O(m \cdot n )$ where $m$ is the number of rows and $n$ is the number of columns of the image.

\section{Theoretical acceleration}

\paragraph*{Hardware} The theoretical limitations depends on the hardware.
In our case, we use a jetson nano which has a max threads/block bound of 1024 and 128 CUDA cores.
Therefore, if we want to accelerate our application properly, we need want to maximize the number of thread running the same operation on the warps.
In our case, we have a lot of small operations in parallel and we therefore we can't use the full power of the GPU because memory transfer will be too long.

If we analyze the GPU time only with nprof on \ref{tab:mlp_cuda_profiler}, we can use Amdahl's law to calculate the theoretical acceleration.
The kernel use 11.69 second, and the memcpy use 15.41 second and the memset use 5.6s. The part we can accelerate is only over those 11.69seconds.
Therefore, because the total GPU time takes more than running on the CPU baseline, we can't accelerate the application with the GPU faster than the cpu.

\paragraph*{Roofline} 

The max performance is 472 GFLOPs and the max memory bandwidth is 25.6G GB/s. Therefore, the jetson is balanced with CI = 18.43.
If we calculate the number of operations on the backpropagation kernel we have: $Y * H * B * 18op = 1.8M$. The number of data transfered is $Y * H * B * 4Bytes = 0.4M$. 
Therefore the CI of our kernel is 4.5. Using the symetry in the roofline model our kernel performance is $ 472 / 18.43 * 4.5 = 115.24 GFLOPs$.
As we can see, we are not limited by the performance but by the memory.

\pagebreak
\assignmentSection{Stage 3 – Acceleration}

\section{Process}
First, we added OpenMP to the base code. As a result, we have a slight performance increase.
Then we wanted to add a CUDA kernel function as requested.
However, the performance was not as good as the original code.
So we spent a lot of time trying to optimize the CUDA code.
We tried to make several CUDA kernel functions, to use cuBLAS, to make memory optimizations.
Unfortunately, we never managed to improve the code base with CUDA
because the transfer between the CPU and the GPU took more time than the CUDA kernel function itself.
Therefore, we decided to make a second application and optimize it with a CUDA kernel function.

\section{Accelerated parts}

\paragraph*{Choice}
We chose to keep the optimization only with OpenMP on MLP because it is more efficient alone than with CUDA optimizations.
To respect the instructions and accelerate part of the code on GPU, we used the CUDA runtime library with CUDA kernel functions which allowed us to have good results.
We looked at using cuBLAS but it was not adapted because it is intended for matrix calculations and it is not really what is done in our example.
We did not use cuDNN because it is not suitable for our needs as it is specifically created for deep neural networks.

\paragraph*{Data} For MLP, we use the Fashion MINST dataset as explained above.
So we will have several large arrays but mostly loops that will go through these arrays.
So we found three triple for loops that took a lot of execution time according to us.
In a first step, we regenerated a single triple loop with these three loops, which will simplify the transition to CUDA kernel. In a second step, we put all the data on the GPU.
Then, we created a CUDA kernel function which allows to replace the generated triple for loop.


\begin{lstlisting}[language=C, caption="Original code with triple loop"]
	/* dy */
	for (int b = 0; b < B; b++)
	  for (int k = 0; k < Y; k++)
		dy[b * Y + k] = p[b * Y + k] - t[b * Y + k];
	/* dv := h * dy' */
	for (int b = 0; b < B; b++)
	  for (int j = 0; j < H; j++)
		for (int k = 0; k < Y; k++)
		  dv[k * H + j] += h[b * H + j] * dy[b * Y + k];
	/* dh := v * dy */
	for (int b = 0; b < B; b++)
	  for (int j = 0; j < H; j++)
		for (int k = 0; k < Y; k++)
		  dh[b * H + j] += v[k * H + j] * dy[b * Y + k];
\end{lstlisting}


\begin{lstlisting}[language=C, caption="Edited nested triple loop"]
	/* dy */
	for (int b = 0; b < B; b++)
	  for (int k = 0; k < Y; k++)
		for (int j = 0; j < H; j++)
		{
		  dy[b * Y + k] = p[b * Y + k] - t[b * Y + k];
		  dv[k * H + j] += h[b * H + j] * dy[b * Y + k];
		  dh[b * H + j] += v[k * H + j] * dy[b * Y + k];
		}
\end{lstlisting}

\begin{lstlisting}[language=C, caption="CUDA kernel function to calculate back\_propagation"]
	// Cuda kernel function of loop
	__global__ void backprop_kernel(float *p, float *t, float *dv, float *v, float *dh, float *h)
	{
		int y_idx = threadIdx.x + blockIdx.x * blockDim.x;
		int h_idx = threadIdx.y + blockIdx.y * blockDim.y;
	
		if (y_idx >= H || h_idx >= Y)
			return;
	
		float dy, dh_gpu, dv_gpu;
	
		for(int b_idx = 0; b_idx < B; b_idx++){
		  dy = p[b_idx * Y + h_idx] - t[b_idx * Y + h_idx];
		  dv_gpu = h[b_idx * H + y_idx] * dy;
		  dh_gpu = v[h_idx * H + y_idx] * dy;
		  atomicAdd(&dv[h_idx * H + y_idx], dv_gpu);
		  atomicAdd(&dh[b_idx * H + y_idx], dh_gpu);
		}
	}
	
	dim3 dimBlock(32, 32, 1);
	dim3 dimGrid(Y / 32 + 1, H / 32 + 1, 1);
	backprop_kernel<<<dimGrid, dimBlock>>>(p_gpu, t_gpu, dv_gpu, v_gpu, dh_gpu, h_gpu);
\end{lstlisting}

As said before, this code is still slower than the original and using OpenMP alone is faster.
So there is an OpenMP directive (\#pragma omp parallel for) before each for loop.
The number of threads is set to 4.

For the application that transforms the image into grayscale, there is a double for loop that runs through the image.
So we created a CUDA kernel function that allows to replace the generated double for loop and thus to use the GPU computing power.

\begin{lstlisting}[language=C, caption="Original grayscale code"]
	for (int y = 0; y < height; y++)
	{
		for (int x = 0; x < width; x++)
		{
			Vec3b color = color_image.at<Vec3b>(y, x);
			int gray = (color[0] + color[1] + color[2]) / 3;
			gray_image.at<unsigned char>(y, x) = gray;
		}
	}
\end{lstlisting}


\begin{lstlisting}[language=C, caption="CUDA kernel function to convert image to grayscale"]
	__global__ void convertToGray(uchar3 *input, unsigned char *output, int width, int height)
	{
		int x = blockIdx.x * blockDim.x + threadIdx.x;
		int y = blockIdx.y * blockDim.y + threadIdx.y;
	
		if (x < width && y < height)
		{
			output[y * width + x] = (input[y * width + x].x + input[y * width + x].y + input[y * width + x].z) / 3;
		}
	}
	
	dim3 block(BLOCK_SIZE, BLOCK_SIZE);
	dim3 grid((width + block.x - 1) / block.x, (height + block.y - 1) / block.y);
	
	convertToGray<<<grid, block>>>(d_color_image, d_gray_image, width, height);
\end{lstlisting}


\pagebreak
\assignmentSection{Stage 4 – Analysis of results}

Given the work you performed in Stage 3, we want you know to analyse the results obtained and draw
some conclusions:
• Analysis of the results: what is the performance enhancement achieved? Where does it come
from and why? What is the new bottleneck created?
• Potential future lines: seeing the acceleration results, what other things should you accelerate?
Now that you see the results, should you have done something different?

\paragraph*{MLP Analysis} As we can see, the OpenMP implementation is the fastest then the baseline and finally the CUDA and OpenMP is the slowest.
The CUDA profiler results are showed on table \ref{tab:mlp_cuda_profiler}.
The CUDA profiler shows that the CUDA memcpy is the most time consuming operation, followed by the backprop\_kernel and the CUDA memset.
So our kernel function is too short to benefit from the GPU acceleration. The cost to move the data to memory are too high.

\paragraph*{Grayscale Analysis}
For the grayscale algorithm, we can see that the acceleration is substantial. For instance, for the winter image the gpu algorithm is 4 time faster.

\begin{table}[h]\centering
	\begin{tabular}{|l|c|c|c|c|c|c|c|}
		\hline
		\textbf{Type} & \textbf{Time perc.} & \textbf{Time} & \textbf{Calls} & \textbf{Avg} & \textbf{Name}         \\ \hline
		GPU :         & 47.10\%             & 15.4163s      & 410754         & 37.531us     & CUDA memcpy HtoH      \\ \hline
		              & 35.72\%             & 11.6917s      & 68459          & 170.78us     & backprop\_kernel      \\ \hline
		              & 17.18\%             & 5.62353s      & 136918         & 41.072us     & CUDA memset           \\ \hline
		API calls:    & 74.08\%             & 145.662s      & 410754         & 354.62us     & cudaMemcpy            \\ \hline
		              & 13.21\%             & 25.9788s      & 136918         & 189.74us     & cudaMemset            \\ \hline
		              & 9.01\%              & 17.7185s      & 68459          & 258.82us     & cudaDeviceSynchronize \\ \hline
		              & 3.49\%              & 6.86282s      & 68459          & 100.25us     & cudaLaunchKernel      \\ \hline
		              & 0.18\%              & 344.80ms      & 7              & 49.257ms     & cudaMallocManaged     \\ \hline
		              & 0.04\%              & 72.091ms      & 68459          & 1.0530us     & cudaGetLastError      \\ \hline
		              & 0.00\%              & 953.77us      & 7              & 136.25us     & cudaFree              \\ \hline
		              & 0.00\%              & 108.55us      & 97             & 1.1190us     & cuDeviceGetAttribute  \\ \hline
		              & 0.00\%              & 10.521us      & 1              & 10.521us     & cuDeviceTotalMem      \\ \hline
		              & 0.00\%              & 7.3430us      & 3              & 2.4470us     & cuDeviceGetCount      \\ \hline
		              & 0.00\%              & 3.8540us      & 2              & 1.9270us     & cuDeviceGet           \\ \hline
		              & 0.00\%              & 1.5100us      & 1              & 1.5100us     & cuDeviceGetName       \\ \hline
		              & 0.00\%              & 938ns         & 1              & 938ns        & cuDeviceGetUuid       \\ \hline
	\end{tabular}
	\caption{MLP CUDA accelerated GPU profiling results}
	\label{tab:mlp_cuda_profiler}
\end{table}

\end{document}
